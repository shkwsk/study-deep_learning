# Deep Learningを浅く理解する
「ゼロから作るDeep Learning」Pythonで学ぶディープラーニングの理論と実装  
[Amazon](https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95Python%E3%81%A7%E5%AD%A6%E3%81%B6%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%A3%85-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873117585/ref=cm_cr_arp_d_product_top?ie=UTF8)

↑全部読めてないので、ニューラルネットワークによる分類・学習について説明します。


## パーセプロトロン
パーセプトロンは複数の信号を入力として受け取り、1つの信号を出力します。  
送られてきた信号の値によって発火するかどうか（信号を通すか通さないか）を表現できます。  

<center><img src="https://raw.githubusercontent.com/shkwsk/study-deep_learning/images/img/perceptron.png" width="300"></center>

パーセプトロンは、入力信号のそれぞれに固有の重みをもち、それぞれの入力がどれくらい重要かコントロールすることができます。
重みが大きければ大きいほど、その信号が重要であるといえます。


## ニューラルネットワーク
ニューラルネットワークはパーセプトロンを使って、分類問題や回帰問題を解くことができます。

* 分類問題: 入力データがどのクラスに属するか（例: 人が写った画像から性別を判定）（離散的）
* 回帰問題: 入力データから数値の予測を行う（例: 人が写った画像から体重を予測）（連続的）

また、重みをデータから自動で学習することができますが、ここでは問題の解き方に焦点を当てます。  
ニューラルネットワークは、以下の図のように表せます。
左のパーセプトロンの列から入力層・中間層・出力層と呼びます。

<center><img src="https://raw.githubusercontent.com/shkwsk/study-deep_learning/images/img/neural_network.png" width="200"></center>

さっきのパーセプトロンを数式で表すと、以下のようになります。  

$$
  y = \Biggl\{
  \begin{aligned}
  \: 0 \:\:\: (b + w_1 x_1 + w_2 x_2 \leqq 0) \\
  \: 1 \:\:\: (b + w_1 x_1 + w_2 x_2 >     0) \\
  \end{aligned} \tag{1}
$$

ここで $b$ は「バイアス」でニューロンの発火のしやすさ、$w_1$ と $w_2$ は「重み」で入力信号の重要性を表します。
これを図で表すと、以下のようになります。

<center><img src="https://raw.githubusercontent.com/shkwsk/study-deep_learning/images/img/perceptron2.png" width="200"></center>

バイアスの入力信号は $1$ で、重みで $b$ をかけているイメージです。こうして表現しておくと後で行列で表すときに便利です。

### 活性化関数
ここで、式 ($1$) を分解して以下のように書き換えます。

$$
  a = b + w_1 x_1 + w_2 x_2 \tag{2.1}
$$
$$
  h(x) = \Biggl\{
  \begin{aligned}
  \: 0 \:\:\: (x \leqq 0) \\
  \: 1 \:\:\: (x >     0) \\
  \end{aligned} \tag{2.2}
$$
$$
  y = h (a) \tag{2.3}
$$

ここで、
式 ($2.2$) は値をみて発火するかどうかを決定する活性化関数といいます。
この式変形で先程の図は次のように表現できます。

<center><img src="https://raw.githubusercontent.com/shkwsk/study-deep_learning/images/img/neuron.png" width="200"></center>

yのニューロンが分解され、入力信号を重みとバイアスで総和（a）し、活性化関数を通すことでyの値が決まります。
式 $(2.2)$ はステップ関数と呼ばれる活性化関数の1つで、活性化関数には他にもいろいろ関数を適用できます。

#### シグモイド関数
シグモイド関数は活性化関数としてよく用いられ、以下の式とグラフで表されます。
$$
  h(x) = \frac{1}{1 + \exp(-x)} \tag{3}
$$
<center><img src="https://raw.githubusercontent.com/shkwsk/study-deep_learning/images/img/sigmoid.png" width="300"></center>
式 $(2.2)$ のステップ関数に比べ、値が滑らかに変化することが分かります。

## forward propagation
入力 -(重み)-> 隠れ層 -(重み)-> 出力層
隠れ層 & 出力層では活性化関数を適用する
重み付けと推論（活性化）を繰り返していくイメージ

### 活性化関数
* ステップ関数
* シグモイド関数
* ReLU（ランプ関数）

出力層によく用いられる
* ソフトマックス関数
* 恒等関数

## ニューラルネットワークによる分類

## バッチ処理
入力データ(N次元)をM個まとめて処理する
出力データ(<分類クラス数>次元)はM個まとめて出力される
データの読み込み処理をM個まとめることができるので、リソースを演算に回すことができる


## ニューラルネットワークの学習
## 損失関数
* 二乗和誤差
* 交差エントロピー誤差

## ミニバッチ学習
N個の訓練データの中からランダムにm個を選んで学習すること

## 勾配
重みパラメータの損失関数を減らす方向のこと
中心差分で数値偏微分を行い勾配を求める

## パラメータの更新
重みパラメータを学習率に合わせて更新する
勾配でパラメータを更新することを勾配降下法という

## 確率的勾配降下法（SGD）
無作為に選び出したデータに対して行う勾配降下法


# 5.誤差逆伝搬法
数値微分で学習する方法は時間がかかるので誤差逆伝搬法を使う

## 連鎖律
連鎖律の原理
合成関数の微分は、合成関数内のそれぞれの関数の微分の積となる

z = t^2, t = x+y のとき、
dz/dx = dz/dt * dt/dz

## 逆伝搬
* 加算レイヤ(x+y): 入力をそのまま返す
* 乗算レイヤ(xy): 入力にひっくり返したものを掛けて返す
レイヤは順方向(f(x,y)=xy)の計算と、逆方向(df(x,y)/dx=y, df(x,y)/dy=x)の計算ができる
順伝搬y=f(x)のとき、逆伝搬はE=E*dy/dx(Eは定数)となる
-> レイヤに活性化関数を適用する
