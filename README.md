# ゼロから作るDeep Learning
Pythonで学ぶディープラーニングの理論と実装

# 3. ニューラルネットワーク
* 分類問題: 入力データがどのクラスに属するか（例: 人が写った画像から性別を判定）（離散的）
* 回帰問題: 入力データから数値の予測を行う（例: 人が写った画像から体重を予測）（連続的）

## forward propagation
入力 -(重み)-> 隠れ層 -(重み)-> 出力層
隠れ層 & 出力層では活性化関数を適用する
重み付けと推論（活性化）を繰り返していくイメージ

## 活性化関数
* ステップ関数
* シグモイド関数
* ReLU（ランプ関数）

出力層によく用いられる
* ソフトマックス関数
* 恒等関数

## バッチ処理
入力データ(N次元)をM個まとめて処理する
出力データ(<分類クラス数>次元)はM個まとめて出力される
データの読み込み処理をM個まとめることができるので、リソースを演算に回すことができる


# 4. ニューラルネットワークの学習
## 損失関数
* 二乗和誤差
* 交差エントロピー誤差

## ミニバッチ学習
N個の訓練データの中からランダムにm個を選んで学習すること

## 勾配
重みパラメータの損失関数を減らす方向のこと
中心差分で数値偏微分を行い勾配を求める

## パラメータの更新
重みパラメータを学習率に合わせて更新する
勾配でパラメータを更新することを勾配降下法という

## 確率的勾配降下法（SGD）
無作為に選び出したデータに対して行う勾配降下法


# 5.誤差逆伝搬法
数値微分で学習する方法は時間がかかるので誤差逆伝搬法を使う

## 連鎖律
連鎖律の原理
合成関数の微分は、合成関数内のそれぞれの関数の微分の積となる

z = t^2, t = x+y のとき、
dz/dx = dz/dt * dt/dz

## 逆伝搬
* 加算レイヤ(x+y): 入力をそのまま返す
* 乗算レイヤ(xy): 入力にひっくり返したものを掛けて返す
レイヤは順方向(f(x,y)=xy)の計算と、逆方向(df(x,y)/dx=y, df(x,y)/dy=x)の計算ができる
順伝搬y=f(x)のとき、逆伝搬はE=E*dy/dx(Eは定数)となる
-> レイヤに活性化関数を適用する
